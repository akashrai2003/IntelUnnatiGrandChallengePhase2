{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5db69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load your lidar point cloud data (replace 'lidar_data.pcd' with your actual .pcd file)\n",
    "lidar_cloud = o3d.io.read_point_cloud('IDD_3D/20220118103808_seq_0/lidar/00000.pcd')\n",
    "\n",
    "# Convert the point cloud to a NumPy array\n",
    "lidar_points = np.asarray(lidar_cloud.points)\n",
    "\n",
    "# Preprocess the point cloud (e.g., remove ground points, downsample)\n",
    "# For simplicity, we assume you've already preprocessed the data\n",
    "\n",
    "# Object detection (DBSCAN clustering)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "labels = dbscan.fit_predict(lidar_points)\n",
    "\n",
    "# Extract unique labels to identify clusters\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Object localization (calculating centroids)\n",
    "object_positions = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        continue  # Skip noise points labeled as -1\n",
    "    cluster_points = lidar_points[labels == label]\n",
    "    centroid = np.mean(cluster_points, axis=0)\n",
    "    object_positions.append(centroid)\n",
    "\n",
    "# Visualize the localized objects\n",
    "visualized_objects = [o3d.geometry.PointCloud()]\n",
    "for pos in object_positions:\n",
    "    object_cloud = o3d.geometry.PointCloud()\n",
    "    object_cloud.points = o3d.utility.Vector3dVector(np.array([pos]))\n",
    "    object_cloud.paint_uniform_color([1, 0, 0])  # Color the object points in red\n",
    "    visualized_objects.append(object_cloud)\n",
    "\n",
    "o3d.visualization.draw_geometries([lidar_cloud] + visualized_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12229489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] [ViewControl] SetViewPoint() failed because window height and width are not set.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Load the LiDAR point cloud data\n",
    "pcd = o3d.io.read_point_cloud(\"IDD_3D/20220118103808_seq_0/lidar/00000.pcd\")\n",
    "\n",
    "# Load 3D bounding box labels from the JSON file\n",
    "# Replace 'labels.json' with the path to your JSON file\n",
    "import json\n",
    "with open('IDD_3D/20220118103808_seq_0/label/00000.json', 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Create a dictionary to map object types to colors for visualization\n",
    "class_to_color = {\n",
    "    \"MotorcycleRider\": [1.0, 0.0, 0.0],  # Red color for MotorcycleRider\n",
    "    \"Pedestrian\": [1.0, 0.0, 0.0],\n",
    "    \"Car\": [0.0, 1.0, 0.0],\n",
    "    \"Scooter\": [0.0, 0.0, 1.0],\n",
    "    \"TourCar\": [0.0, 1.0, 0.0],\n",
    "    \"Truck\": [0.0, 1.0, 0.0]# Add more object types and corresponding colors as needed\n",
    "}\n",
    "\n",
    "# Visualize the LiDAR point cloud with bounding boxes\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "# Extract and visualize objects from the point cloud\n",
    "for label in labels:\n",
    "    obj_type = label[\"obj_type\"]\n",
    "    obj_id = label[\"obj_id\"]\n",
    "    position = label[\"psr\"][\"position\"]\n",
    "    rotation = label[\"psr\"][\"rotation\"]\n",
    "    scale = label[\"psr\"][\"scale\"]\n",
    "\n",
    "    # Create a bounding box for visualization\n",
    "    color = class_to_color.get(obj_type, [0.0, 1.0, 0.0])  # Default to green if not in the dictionary\n",
    "    bbox = o3d.geometry.OrientedBoundingBox()\n",
    "    bbox.center = np.array([position[\"x\"], position[\"y\"], position[\"z\"]])\n",
    "    bbox.R = np.array([[np.cos(rotation[\"z\"]), -np.sin(rotation[\"z\"]), 0],\n",
    "                      [np.sin(rotation[\"z\"]), np.cos(rotation[\"z\"]), 0],\n",
    "                      [0, 0, 1]])\n",
    "    bbox.extent = np.array([scale[\"x\"], scale[\"y\"], scale[\"z\"]])\n",
    "\n",
    "    # Visualize the bounding box in the point cloud\n",
    "    o3d.visualization.draw_geometries([pcd, bbox])\n",
    "\n",
    "# Additional processing and object detection logic can be added here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6402616",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Open3D was not built with PyTorch support!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mml3d\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gpu\\lib\\site-packages\\open3d\\ml\\torch\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _build_config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _build_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPytorch_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen3D was not built with PyTorch support!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m _o3d_torch_version \u001b[38;5;241m=\u001b[39m _verp(_build_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPytorch_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check match with PyTorch version, any patch level is OK\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Open3D was not built with PyTorch support!"
     ]
    }
   ],
   "source": [
    "import open3d.ml.torch as ml3d\n",
    "import open3d.ml.torch.models\n",
    "import open3d.ml.torch.pipelines\n",
    "import open3d.ml.torch.datasets\n",
    "\n",
    "# Load and preprocess your LiDAR dataset\n",
    "dataset = open3d.ml.torch.datasets.SemanticKITTI(\n",
    "    root=\"path/to/dataset\", pipeline=\"voxel\",\n",
    ")\n",
    "\n",
    "# Create a PointNet model\n",
    "model = open3d.ml.torch.models.PointNet2SSG(\n",
    "    num_classes=dataset.num_classes,\n",
    "    in_channels=dataset.num_features,\n",
    ")\n",
    "\n",
    "# Train the model (you would typically have a separate training dataset)\n",
    "trainer = open3d.ml.torch.pipelines.Trainer(model, dataset)\n",
    "trainer.train()\n",
    "\n",
    "# Perform object detection and classification on a new point cloud\n",
    "new_point_cloud = o3d.io.read_point_cloud(\"IDD_3D/20220118103808_seq_0/lidar/00000.pcd\") # Load or capture your new point cloud\n",
    "detections = model.inference(new_point_cloud)\n",
    "\n",
    "# Visualize the results\n",
    "detections.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317031be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b65c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06b2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d500c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance to C++ type (compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         rotation \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsr\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Create a bounding box using Open3D\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m         box \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mOrientedBoundingBox(center\u001b[38;5;241m=\u001b[39m\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVector3dVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     20\u001b[0m                                                R\u001b[38;5;241m=\u001b[39mo3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector([rotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], rotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], rotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]]),\n\u001b[0;32m     21\u001b[0m                                                extent\u001b[38;5;241m=\u001b[39mo3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector([scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]]))\n\u001b[0;32m     23\u001b[0m         bounding_boxes\u001b[38;5;241m.\u001b[39mappend(box)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the point cloud data\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to cast Python instance to C++ type (compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"IDD_3D/20220118103808_seq_0/label/00000.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Initialize an empty list to store bounding box information\n",
    "bounding_boxes = []\n",
    "\n",
    "# Iterate through the JSON data and extract bounding box info for cars\n",
    "for entry in data:\n",
    "    if entry[\"obj_type\"] == \"Car\":\n",
    "        position = entry[\"psr\"][\"position\"]\n",
    "        scale = entry[\"psr\"][\"scale\"]\n",
    "        rotation = entry[\"psr\"][\"rotation\"]\n",
    "\n",
    "        # Create a bounding box using Open3D\n",
    "        box = o3d.geometry.OrientedBoundingBox(center=o3d.utility.Vector3dVector([position[\"x\"], position[\"y\"], position[\"z\"]]),\n",
    "                                               R=o3d.utility.Vector3dVector([rotation[\"x\"], rotation[\"y\"], rotation[\"z\"]]),\n",
    "                                               extent=o3d.utility.Vector3dVector([scale[\"x\"], scale[\"y\"], scale[\"z\"]]))\n",
    "\n",
    "        bounding_boxes.append(box)\n",
    "\n",
    "# Load the point cloud data\n",
    "point_cloud = o3d.io.read_point_cloud(\"IDD_3D/20220118103808_seq_0/lidar/00000.pcd\")\n",
    "\n",
    "# Create a KDTree for point cloud\n",
    "kdtree = o3d.geometry.KDTreeFlann(point_cloud)\n",
    "\n",
    "# Iterate through bounding boxes and count points within each box\n",
    "num_vehicles = 0\n",
    "for box in bounding_boxes:\n",
    "    indices = kdtree.query_ball_point(box.get_center(), max(box.get_extent()))\n",
    "    num_points_in_box = len(indices)\n",
    "    if num_points_in_box >= min_threshold:  # Adjust min_threshold as needed\n",
    "        num_vehicles += 1\n",
    "\n",
    "print(\"Number of vehicles:\", num_vehicles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8b1842",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_rotation_matrix_from_xyz(): incompatible function arguments. The following argument types are supported:\n    1. (rotation: numpy.ndarray[numpy.float64[3, 1]]) -> numpy.ndarray[numpy.float64[3, 3]]\n\nInvoked with: {'x': 0, 'y': 0, 'z': -1.7533384818324207}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create a bounding box using Open3D\u001b[39;00m\n\u001b[0;32m     19\u001b[0m center \u001b[38;5;241m=\u001b[39m [position[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], position[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], position[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 20\u001b[0m rotation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rotation_matrix_from_xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m extent \u001b[38;5;241m=\u001b[39m [scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m], scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m], scale[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     22\u001b[0m box \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mOrientedBoundingBox(center\u001b[38;5;241m=\u001b[39mcenter, R\u001b[38;5;241m=\u001b[39mrotation_matrix, extent\u001b[38;5;241m=\u001b[39mextent)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_rotation_matrix_from_xyz(): incompatible function arguments. The following argument types are supported:\n    1. (rotation: numpy.ndarray[numpy.float64[3, 1]]) -> numpy.ndarray[numpy.float64[3, 3]]\n\nInvoked with: {'x': 0, 'y': 0, 'z': -1.7533384818324207}"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"IDD_3D/20220118103808_seq_0/label/00000.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Initialize an empty list to store bounding box information\n",
    "bounding_boxes = []\n",
    "\n",
    "# Iterate through the JSON data and extract bounding box info for cars\n",
    "for entry in data:\n",
    "    if entry[\"obj_type\"] == \"Car\":\n",
    "        position = entry[\"psr\"][\"position\"]\n",
    "        scale = entry[\"psr\"][\"scale\"]\n",
    "        rotation = entry[\"psr\"][\"rotation\"]\n",
    "\n",
    "        # Create a bounding box using Open3D\n",
    "        center = [position[\"x\"], position[\"y\"], position[\"z\"]]\n",
    "        rotation_matrix = o3d.geometry.get_rotation_matrix_from_xyz(rotation)\n",
    "        extent = [scale[\"x\"], scale[\"y\"], scale[\"z\"]]\n",
    "        box = o3d.geometry.OrientedBoundingBox(center=center, R=rotation_matrix, extent=extent)\n",
    "\n",
    "        bounding_boxes.append(box)\n",
    "\n",
    "# Load the point cloud data\n",
    "point_cloud = o3d.io.read_point_cloud(\"IDD_3D/20220118103808_seq_0/lidar/00000.pcd\")\n",
    "\n",
    "# Create a KDTree for point cloud\n",
    "kdtree = o3d.geometry.KDTreeFlann(point_cloud)\n",
    "\n",
    "# Iterate through bounding boxes and count points within each box\n",
    "num_vehicles = 0\n",
    "min_threshold = 10  # Adjust as needed\n",
    "for box in bounding_boxes:\n",
    "    indices = kdtree.query_radius(center=box.get_center(), radius=max(box.get_extent()))\n",
    "    num_points_in_box = len(indices[0])\n",
    "    if num_points_in_box >= min_threshold:\n",
    "        num_vehicles += 1\n",
    "\n",
    "print(\"Number of vehicles:\", num_vehicles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90d41320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vehicles: 0\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"IDD_3D/20220118103808_seq_0/label/00000.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Initialize an empty list to store bounding box information\n",
    "bounding_boxes = []\n",
    "\n",
    "# Iterate through the JSON data and extract bounding box info for cars\n",
    "for entry in data:\n",
    "    if entry[\"obj_type\"] == \"Car\":\n",
    "        position = entry[\"psr\"][\"position\"]\n",
    "        scale = entry[\"psr\"][\"scale\"]\n",
    "        rotation = entry[\"psr\"][\"rotation\"]\n",
    "\n",
    "        # Create a rotation matrix from the dictionary values\n",
    "        rotation_matrix = np.array([[np.cos(rotation[\"z\"]), -np.sin(rotation[\"z\"]), 0],\n",
    "                                    [np.sin(rotation[\"z\"]), np.cos(rotation[\"z\"]), 0],\n",
    "                                    [0, 0, 1]])\n",
    "\n",
    "        # Create a bounding box using Open3D\n",
    "        center = [position[\"x\"], position[\"y\"], position[\"z\"]]\n",
    "        extent = [scale[\"x\"], scale[\"y\"], scale[\"z\"]]\n",
    "        box = o3d.geometry.OrientedBoundingBox(center=center, R=rotation_matrix, extent=extent)\n",
    "\n",
    "        bounding_boxes.append(box)\n",
    "\n",
    "# Load the point cloud data\n",
    "point_cloud = o3d.io.read_point_cloud(\"IDD_3D/20220118103808_seq_0/lidar/00000.pcd\")\n",
    "\n",
    "# Extract the point cloud as a NumPy array\n",
    "point_cloud_data = np.asarray(point_cloud.points)\n",
    "\n",
    "# Create a NearestNeighbors object for point cloud\n",
    "kdtree = NearestNeighbors(radius=max(max(box.extent) for box in bounding_boxes))\n",
    "kdtree.fit(point_cloud_data)\n",
    "\n",
    "# Iterate through bounding boxes and count points within each box\n",
    "num_vehicles = 0\n",
    "min_threshold = 10  # Adjust as needed\n",
    "for box in bounding_boxes:\n",
    "    indices = kdtree.radius_neighbors([box.center], max(box.extent))\n",
    "    num_points_in_box = len(indices[0])\n",
    "    if num_points_in_box >= min_threshold:\n",
    "        num_vehicles += 1\n",
    "\n",
    "print(\"Number of vehicles:\", num_vehicles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df24f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ceb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87cbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec8ab580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filterpy\n",
      "  Downloading filterpy-1.4.5.zip (177 kB)\n",
      "                                              0.0/178.0 kB ? eta -:--:--\n",
      "                                              0.0/178.0 kB ? eta -:--:--\n",
      "     --                                       10.2/178.0 kB ? eta -:--:--\n",
      "     --                                       10.2/178.0 kB ? eta -:--:--\n",
      "     ------                                30.7/178.0 kB 220.2 kB/s eta 0:00:01\n",
      "     --------                              41.0/178.0 kB 219.4 kB/s eta 0:00:01\n",
      "     -----------------                     81.9/178.0 kB 353.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 178.0/178.0 kB 671.0 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from filterpy) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from filterpy) (1.11.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\akash\\appdata\\roaming\\python\\python39\\site-packages (from filterpy) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (9.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from matplotlib->filterpy) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->filterpy) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akash\\.conda\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n",
      "Building wheels for collected packages: filterpy\n",
      "  Building wheel for filterpy (setup.py): started\n",
      "  Building wheel for filterpy (setup.py): finished with status 'done'\n",
      "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110542 sha256=97d4a6d12c6f493a3f1ac0bf5a1814e2d71e71edea2e8b323c245624bb8b0f1b\n",
      "  Stored in directory: c:\\users\\akash\\appdata\\local\\pip\\cache\\wheels\\53\\e6\\de\\a09ea01e923aaf88b9f8c7c44329e857b2c1a31901167e55e6\n",
      "Successfully built filterpy\n",
      "Installing collected packages: filterpy\n",
      "Successfully installed filterpy-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15dedaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vehicles: 522\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize Kalman Filter for object tracking\n",
    "def init_kalman_filter():\n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "    kf.F = np.array([[1, 1, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 1, 1],\n",
    "                    [0, 0, 0, 1]])\n",
    "    kf.H = np.array([[1, 0, 0, 0],\n",
    "                    [0, 0, 1, 0]])\n",
    "    kf.x = np.array([0, 0, 0, 0])\n",
    "    kf.P *= 1\n",
    "    return kf\n",
    "\n",
    "# Perform object detection in each camera image\n",
    "def detect_objects(image, cascade_classifier):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    objects = cascade_classifier.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return objects\n",
    "\n",
    "# Initialize Kalman filters for object tracking\n",
    "kalman_filters = [init_kalman_filter() for _ in range(6)]\n",
    "\n",
    "# Initialize object tracks\n",
    "object_tracks = defaultdict(list)\n",
    "\n",
    "# Initialize Haar Cascade Classifier for vehicle detection\n",
    "vehicle_cascade = cv2.CascadeClassifier(\"haarcascade_car.xml\")\n",
    "\n",
    "# Process camera images\n",
    "for camera_id in range(6):\n",
    "    for frame_id in range(100):\n",
    "        image = cv2.imread(f\"IDD_3D/20220118103808_seq_0/camera/cam{camera_id}/{frame_id:05d}.png\")\n",
    "        detected_objects = detect_objects(image, vehicle_cascade)\n",
    "        \n",
    "        for obj in detected_objects:\n",
    "            obj_id = len(object_tracks[camera_id])  # Assign a new ID\n",
    "            kalman_filter = kalman_filters[camera_id]\n",
    "            \n",
    "            # Initial state estimation based on detection\n",
    "            x, y, w, h = obj\n",
    "            center_x = x + w // 2\n",
    "            center_y = y + h // 2\n",
    "            measurement = np.array([center_x, center_y])\n",
    "            \n",
    "            kalman_filter.x = np.array([center_x, 0, center_y, 0])\n",
    "            kalman_filter.P *= 1\n",
    "            \n",
    "            kalman_filter.update(measurement)\n",
    "            object_tracks[camera_id].append({'id': obj_id, 'kalman_filter': kalman_filter})\n",
    "            \n",
    "        # Draw object tracks on the image\n",
    "        for obj in object_tracks[camera_id]:\n",
    "            x, y = obj['kalman_filter'].x[:2]\n",
    "            cv2.circle(image, (int(x), int(y)), 3, (0, 255, 0), -1)\n",
    "        \n",
    "        cv2.imshow(f\"Camera {camera_id}\", image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "# Merge object tracks from all cameras and count vehicles\n",
    "all_object_tracks = []\n",
    "for camera_id in range(6):\n",
    "    all_object_tracks.extend(object_tracks[camera_id])\n",
    "\n",
    "# Count the number of unique vehicles\n",
    "vehicle_count = len(set(obj['id'] for obj in all_object_tracks))\n",
    "\n",
    "print(f\"Total number of vehicles: {vehicle_count}\")\n",
    "\n",
    "# Release the OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811e1a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane: 0\n",
      "bicycle: 0\n",
      "bird: 0\n",
      "boat: 3\n",
      "bottle: 0\n",
      "bus: 18\n",
      "car: 448\n",
      "cat: 0\n",
      "chair: 2\n",
      "cow: 0\n",
      "diningtable: 0\n",
      "dog: 0\n",
      "horse: 0\n",
      "motorbike: 16\n",
      "person: 149\n",
      "pottedplant: 1\n",
      "sheep: 0\n",
      "sofa: 0\n",
      "train: 0\n",
      "tvmonitor: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the YOLO model for object detection\n",
    "yolo_net = cv2.dnn.readNet(r\"YOLO_files/yolo-voc.weights\", \"YOLO_files/yolo-voc.cfg\")\n",
    "\n",
    "# Load class labels for vehicle categories\n",
    "with open(\"YOLO_files/voc_classes.txt\") as f:\n",
    "    class_labels = f.read().strip().split('\\n')\n",
    "\n",
    "# Function to detect and classify vehicles in an image using YOLO\n",
    "def detect_and_classify_vehicles_yolo(image):\n",
    "    # Preprocess the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    \n",
    "    # Get the YOLO model's output layer names\n",
    "    output_layers = yolo_net.getUnconnectedOutLayersNames()\n",
    "    \n",
    "    # Run YOLO forward pass\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "    \n",
    "    vehicle_categories = []\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # Adjust confidence threshold as needed\n",
    "                label = class_labels[class_id]\n",
    "                vehicle_categories.append(label)\n",
    "    \n",
    "    return vehicle_categories\n",
    "\n",
    "# Path to the directory containing images (seq_5)\n",
    "image_dir = \"IDD_3D/20220118103808_seq_5/camera\"\n",
    "\n",
    "# Number of frames in seq_5\n",
    "num_frames = 100\n",
    "\n",
    "# Initialize a dictionary to store the counts of different vehicle categories\n",
    "vehicle_counts = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 0,\n",
    "    'bird': 0,\n",
    "    'boat': 0,\n",
    "    'bottle': 0,\n",
    "    'bus': 0,\n",
    "    'car': 0,\n",
    "    'cat': 0,\n",
    "    'chair': 0,\n",
    "    'cow': 0,\n",
    "    'diningtable': 0,\n",
    "    'dog': 0,\n",
    "    'horse': 0,\n",
    "    'motorbike': 0,\n",
    "    'person': 0,\n",
    "    'pottedplant': 0,\n",
    "    'sheep': 0,\n",
    "    'sofa': 0,\n",
    "    'train': 0,\n",
    "    'tvmonitor': 0\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Iterate through the frames (00500.png to 00599.png)\n",
    "for frame_id in range(500, 600):\n",
    "    for cam_id in range(6):\n",
    "        # Construct the file path for the current image\n",
    "        image_path = f\"{image_dir}/cam{cam_id}/{frame_id:05d}.png\"\n",
    "\n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Detect and classify vehicles in the image using YOLO\n",
    "        vehicle_categories = detect_and_classify_vehicles_yolo(image)\n",
    "\n",
    "        # Update the counts based on vehicle categories\n",
    "        for category in vehicle_categories:\n",
    "            vehicle_counts[category] += 1\n",
    "\n",
    "# Print the counts of different vehicle categories\n",
    "for category, count in vehicle_counts.items():\n",
    "    print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a9f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Total Vehicles = 18\n",
      "Frame 1: Total Vehicles = 21\n",
      "Frame 2: Total Vehicles = 20\n",
      "Frame 3: Total Vehicles = 23\n",
      "Frame 4: Total Vehicles = 17\n",
      "Frame 5: Total Vehicles = 14\n",
      "Frame 6: Total Vehicles = 18\n",
      "Frame 7: Total Vehicles = 18\n",
      "Frame 8: Total Vehicles = 25\n",
      "Frame 9: Total Vehicles = 21\n",
      "Frame 10: Total Vehicles = 18\n",
      "Frame 11: Total Vehicles = 15\n",
      "Frame 12: Total Vehicles = 20\n",
      "Frame 13: Total Vehicles = 22\n",
      "Frame 14: Total Vehicles = 20\n",
      "Frame 15: Total Vehicles = 21\n",
      "Frame 16: Total Vehicles = 21\n",
      "Frame 17: Total Vehicles = 17\n",
      "Frame 18: Total Vehicles = 16\n",
      "Frame 19: Total Vehicles = 21\n",
      "Frame 20: Total Vehicles = 12\n",
      "Frame 21: Total Vehicles = 14\n",
      "Frame 22: Total Vehicles = 18\n",
      "Frame 23: Total Vehicles = 13\n",
      "Frame 24: Total Vehicles = 10\n",
      "Frame 25: Total Vehicles = 13\n",
      "Frame 26: Total Vehicles = 10\n",
      "Frame 27: Total Vehicles = 12\n",
      "Frame 28: Total Vehicles = 11\n",
      "Frame 29: Total Vehicles = 12\n",
      "Frame 30: Total Vehicles = 14\n",
      "Frame 31: Total Vehicles = 16\n",
      "Frame 32: Total Vehicles = 14\n",
      "Frame 33: Total Vehicles = 12\n",
      "Frame 34: Total Vehicles = 15\n",
      "Frame 35: Total Vehicles = 13\n",
      "Frame 36: Total Vehicles = 10\n",
      "Frame 37: Total Vehicles = 14\n",
      "Frame 38: Total Vehicles = 16\n",
      "Frame 39: Total Vehicles = 20\n",
      "Frame 40: Total Vehicles = 17\n",
      "Frame 41: Total Vehicles = 16\n",
      "Frame 42: Total Vehicles = 14\n",
      "Frame 43: Total Vehicles = 12\n",
      "Frame 44: Total Vehicles = 12\n",
      "Frame 45: Total Vehicles = 14\n",
      "Frame 46: Total Vehicles = 21\n",
      "Frame 47: Total Vehicles = 17\n",
      "Frame 48: Total Vehicles = 17\n",
      "Frame 49: Total Vehicles = 18\n",
      "Frame 50: Total Vehicles = 11\n",
      "Frame 51: Total Vehicles = 11\n",
      "Frame 52: Total Vehicles = 9\n",
      "Frame 53: Total Vehicles = 12\n",
      "Frame 54: Total Vehicles = 14\n",
      "Frame 55: Total Vehicles = 15\n",
      "Frame 56: Total Vehicles = 15\n",
      "Frame 57: Total Vehicles = 19\n",
      "Frame 58: Total Vehicles = 17\n",
      "Frame 59: Total Vehicles = 14\n",
      "Frame 60: Total Vehicles = 16\n",
      "Frame 61: Total Vehicles = 18\n",
      "Frame 62: Total Vehicles = 17\n",
      "Frame 63: Total Vehicles = 13\n",
      "Frame 64: Total Vehicles = 16\n",
      "Frame 65: Total Vehicles = 16\n",
      "Frame 66: Total Vehicles = 11\n",
      "Frame 67: Total Vehicles = 12\n",
      "Frame 68: Total Vehicles = 14\n",
      "Frame 69: Total Vehicles = 17\n",
      "Frame 70: Total Vehicles = 16\n",
      "Frame 71: Total Vehicles = 12\n",
      "Frame 72: Total Vehicles = 10\n",
      "Frame 73: Total Vehicles = 12\n",
      "Frame 74: Total Vehicles = 13\n",
      "Frame 75: Total Vehicles = 14\n",
      "Frame 76: Total Vehicles = 14\n",
      "Frame 77: Total Vehicles = 15\n",
      "Frame 78: Total Vehicles = 12\n",
      "Frame 79: Total Vehicles = 14\n",
      "Frame 80: Total Vehicles = 11\n",
      "Frame 81: Total Vehicles = 10\n",
      "Frame 82: Total Vehicles = 11\n",
      "Frame 83: Total Vehicles = 10\n",
      "Frame 84: Total Vehicles = 10\n",
      "Frame 85: Total Vehicles = 14\n",
      "Frame 86: Total Vehicles = 17\n",
      "Frame 87: Total Vehicles = 15\n",
      "Frame 88: Total Vehicles = 18\n",
      "Frame 89: Total Vehicles = 16\n",
      "Frame 90: Total Vehicles = 19\n",
      "Frame 91: Total Vehicles = 21\n",
      "Frame 92: Total Vehicles = 15\n",
      "Frame 93: Total Vehicles = 20\n",
      "Frame 94: Total Vehicles = 29\n",
      "Frame 95: Total Vehicles = 29\n",
      "Frame 96: Total Vehicles = 31\n",
      "Frame 97: Total Vehicles = 34\n",
      "Frame 98: Total Vehicles = 30\n",
      "Frame 99: Total Vehicles = 29\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize Haar Cascade Classifier for vehicle detection\n",
    "vehicle_cascade = cv2.CascadeClassifier(\"haarcascade_car.xml\")\n",
    "\n",
    "# Function to detect and count vehicles in an image\n",
    "def detect_and_count_vehicles(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    vehicles = vehicle_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return len(vehicles)\n",
    "\n",
    "# Path to the directory containing images in seq_5\n",
    "image_dir = \"IDD_3D/20220118103808_seq_5/camera\"\n",
    "\n",
    "# Number of frames in seq_5\n",
    "num_frames = 100\n",
    "\n",
    "# Initialize a dictionary to store the vehicle counts for each frame\n",
    "vehicle_counts = {}\n",
    "\n",
    "# Iterate through the frames\n",
    "for frame_id in range(num_frames):\n",
    "    total_count = 0  # Initialize the total count for the current frame\n",
    "    for cam_id in range(6):\n",
    "        # Construct the file path for the current image\n",
    "        image_path = f\"{image_dir}/cam{cam_id}/{frame_id+500:05d}.png\"\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Detect and count vehicles in the image\n",
    "        num_vehicles = detect_and_count_vehicles(image)\n",
    "        \n",
    "        # Add the count for the current camera view to the total count\n",
    "        total_count += num_vehicles\n",
    "\n",
    "    # Store the total count for the current frame\n",
    "    vehicle_counts[f\"Frame {frame_id}\"] = total_count\n",
    "\n",
    "# Print the total vehicle counts for each frame in seq_5\n",
    "for frame, count in vehicle_counts.items():\n",
    "    print(f\"{frame}: Total Vehicles = {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebade614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Total Vehicles = 18\n",
      "Frame 1: Total Vehicles = 21\n",
      "Frame 2: Total Vehicles = 20\n",
      "Frame 3: Total Vehicles = 23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, cam_image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(frame_images):\n\u001b[0;32m     56\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Camera \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cam_image)\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Close OpenCV windows\u001b[39;00m\n\u001b[0;32m     61\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize Haar Cascade Classifier for vehicle detection\n",
    "vehicle_cascade = cv2.CascadeClassifier(\"haarcascade_car.xml\")\n",
    "\n",
    "# Function to detect and count vehicles in an image\n",
    "def detect_and_count_vehicles(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    vehicles = vehicle_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw bounding boxes around detected vehicles\n",
    "    for (x, y, w, h) in vehicles:\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    return len(vehicles), image\n",
    "\n",
    "# Path to the directory containing images in seq_5\n",
    "image_dir = \"IDD_3D/20220118103808_seq_5/camera\"\n",
    "\n",
    "# Number of frames in seq_5\n",
    "num_frames = 100\n",
    "\n",
    "# Initialize a dictionary to store the vehicle counts and images for each frame\n",
    "vehicle_info = {}\n",
    "\n",
    "# Iterate through the frames\n",
    "for frame_id in range(num_frames):\n",
    "    total_count = 0  # Initialize the total count for the current frame\n",
    "    frame_images = []  # Initialize a list to store individual camera images\n",
    "    \n",
    "    for cam_id in range(6):\n",
    "        # Construct the file path for the current image\n",
    "        image_path = f\"{image_dir}/cam{cam_id}/{frame_id+500:05d}.png\"\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Detect and count vehicles in the image and get the image with bounding boxes\n",
    "        num_vehicles, image_with_boxes = detect_and_count_vehicles(image)\n",
    "        \n",
    "        # Add the count for the current camera view to the total count\n",
    "        total_count += num_vehicles\n",
    "        \n",
    "        # Store the individual camera image with bounding boxes\n",
    "        frame_images.append(image_with_boxes)\n",
    "    \n",
    "    # Store the total count and individual camera images\n",
    "    vehicle_info[f\"Frame {frame_id}\"] = (total_count, frame_images)\n",
    "\n",
    "# Print the total vehicle counts for each frame in seq_5\n",
    "for frame, (count, frame_images) in vehicle_info.items():\n",
    "    print(f\"{frame}: Total Vehicles = {count}\")\n",
    "    \n",
    "    # Show the individual camera images with bounding boxes\n",
    "    for i, cam_image in enumerate(frame_images):\n",
    "        cv2.imshow(f\"{frame} - Camera {i}\", cam_image)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "# Close OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35696b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle: 2\n",
      "car: 309\n",
      "motorbike: 68\n",
      "person: 126\n",
      "pottedplant: 1\n",
      "train: 17\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLO model for object detection\n",
    "yolo_net = cv2.dnn.readNet(r\"YOLO_files/yolo-voc.weights\", \"YOLO_files/yolo-voc.cfg\")\n",
    "\n",
    "# Load class labels for vehicle categories\n",
    "with open(\"YOLO_files/voc_classes.txt\") as f:\n",
    "    class_labels = f.read().strip().split('\\n')\n",
    "\n",
    "# Function to detect and classify vehicles in an image using YOLO\n",
    "def detect_and_classify_vehicles_yolo(image):\n",
    "    # Preprocess the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    \n",
    "    # Get the YOLO model's output layer names\n",
    "    output_layers = yolo_net.getUnconnectedOutLayersNames()\n",
    "    \n",
    "    # Run YOLO forward pass\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "    \n",
    "    vehicle_categories = []\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # Adjust confidence threshold as needed\n",
    "                label = class_labels[class_id]\n",
    "                vehicle_categories.append(label)\n",
    "    \n",
    "    return vehicle_categories\n",
    "\n",
    "# Path to the directory containing images\n",
    "image_dir = \"IDD_3D/20220118103808_seq_0/camera\"\n",
    "\n",
    "# Number of frames\n",
    "num_frames = 100\n",
    "\n",
    "# Initialize a dictionary to store the counts of different vehicle categories\n",
    "vehicle_counts = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 0,\n",
    "    'bird': 0,\n",
    "    'boat': 0,\n",
    "    'bottle': 0,\n",
    "    'bus': 0,\n",
    "    'car': 0,\n",
    "    'cat': 0,\n",
    "    'chair': 0,\n",
    "    'cow': 0,\n",
    "    'diningtable': 0,\n",
    "    'dog': 0,\n",
    "    'horse': 0,\n",
    "    'motorbike': 0,\n",
    "    'person': 0,\n",
    "    'pottedplant': 0,\n",
    "    'sheep': 0,\n",
    "    'sofa': 0,\n",
    "    'train': 0,\n",
    "    'tvmonitor': 0\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Iterate through all frames\n",
    "for frame_id in range(num_frames):\n",
    "    for cam_id in range(6):\n",
    "        # Construct the file path for the current image\n",
    "        image_path = f\"{image_dir}/cam{cam_id}/{frame_id:05d}.png\"\n",
    "\n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Detect and classify vehicles in the image using YOLO\n",
    "        vehicle_categories = detect_and_classify_vehicles_yolo(image)\n",
    "\n",
    "        # Update the counts based on vehicle categories\n",
    "        for category in vehicle_categories:\n",
    "            vehicle_counts[category] += 1\n",
    "\n",
    "# Print the counts of different vehicle categories with counts greater than 0\n",
    "for category, count in vehicle_counts.items():\n",
    "    if count > 1:\n",
    "        print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca024030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car: 2\n",
      "motorbike: 1\n",
      "person: 5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the YOLO model for object detection\n",
    "yolo_net = cv2.dnn.readNet(r\"YOLO_files/yolo-voc.weights\", \"YOLO_files/yolo-voc.cfg\")\n",
    "\n",
    "# Load class labels for vehicle categories\n",
    "with open(\"YOLO_files/voc_classes.txt\") as f:\n",
    "    class_labels = f.read().strip().split('\\n')\n",
    "\n",
    "# Function to detect and classify vehicles in an image using YOLO\n",
    "def detect_and_classify_vehicles_yolo(image):\n",
    "    # Preprocess the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    \n",
    "    # Get the YOLO model's output layer names\n",
    "    output_layers = yolo_net.getUnconnectedOutLayersNames()\n",
    "    \n",
    "    # Run YOLO forward pass\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "    \n",
    "    vehicle_categories = []\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # Adjust confidence threshold as needed\n",
    "                label = class_labels[class_id]\n",
    "                vehicle_categories.append(label)\n",
    "    \n",
    "    return vehicle_categories\n",
    "\n",
    "# Path to the directory containing images (seq_0)\n",
    "image_dir = \"IDD_3D/20220118103808_seq_1/camera\"\n",
    "\n",
    "# Initialize a dictionary to store the counts of different vehicle categories\n",
    "vehicle_counts = {\n",
    "    'bicycle': 0,\n",
    "    'bus': 0,\n",
    "    'car': 0,\n",
    "    'motorbike': 0,\n",
    "    'person': 0,\n",
    "    'train': 0\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Iterate through cam0 to cam5 for \"00000.png\"\n",
    "for cam_id in range(6):\n",
    "    # Construct the file path for the current image\n",
    "    image_path = f\"{image_dir}/cam{cam_id}/00100.png\"\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect and classify vehicles in the image using YOLO\n",
    "    vehicle_categories = detect_and_classify_vehicles_yolo(image)\n",
    "\n",
    "    # Update the counts based on vehicle categories\n",
    "    for category in vehicle_categories:\n",
    "        vehicle_counts[category] += 1\n",
    "\n",
    "# Print the counts of different vehicle categories with counts greater than 0\n",
    "for category, count in vehicle_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c63de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car: 2\n",
      "motorbike: 1\n",
      "person: 5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the YOLO model for object detection\n",
    "yolo_net = cv2.dnn.readNet(r\"YOLO_files/yolo-voc.weights\", \"YOLO_files/yolo-voc.cfg\")\n",
    "\n",
    "# Load class labels for vehicle categories\n",
    "with open(\"YOLO_files/voc_classes.txt\") as f:\n",
    "    class_labels = f.read().strip().split('\\n')\n",
    "\n",
    "# Function to detect and classify vehicles in an image using YOLO\n",
    "def detect_and_classify_vehicles_yolo(image):\n",
    "    # Preprocess the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    \n",
    "    # Get the YOLO model's output layer names\n",
    "    output_layers = yolo_net.getUnconnectedOutLayersNames()\n",
    "    \n",
    "    # Run YOLO forward pass\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "    \n",
    "    vehicle_categories = []\n",
    "    vehicle_bboxes = []  # List to store bounding boxes\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # Adjust confidence threshold as needed\n",
    "                label = class_labels[class_id]\n",
    "                vehicle_categories.append(label)\n",
    "                \n",
    "                # Get bounding box coordinates\n",
    "                center_x, center_y, width, height = map(int, obj[0:4] * [image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                x = center_x - width // 2\n",
    "                y = center_y - height // 2\n",
    "                \n",
    "                vehicle_bboxes.append((x, y, width, height))\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    for bbox in vehicle_bboxes:\n",
    "        x, y, width, height = bbox\n",
    "        cv2.rectangle(image, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "    \n",
    "    return vehicle_categories, image\n",
    "\n",
    "# Path to the directory containing images (seq_0)\n",
    "image_dir = \"IDD_3D/20220118103808_seq_1/camera\"\n",
    "\n",
    "# Initialize a dictionary to store the counts of different vehicle categories\n",
    "vehicle_counts = {\n",
    "    'bicycle': 0,\n",
    "    'bus': 0,\n",
    "    'car': 0,\n",
    "    'motorbike': 0,\n",
    "    'person': 0,\n",
    "    'train': 0\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Iterate through cam0 to cam5 for \"00100.png\"\n",
    "for cam_id in range(6):\n",
    "    # Construct the file path for the current image\n",
    "    image_path = f\"{image_dir}/cam{cam_id}/00100.png\"\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect and classify vehicles in the image using YOLO\n",
    "    vehicle_categories, image_with_boxes = detect_and_classify_vehicles_yolo(image)\n",
    "\n",
    "    # Update the counts based on vehicle categories\n",
    "    for category in vehicle_categories:\n",
    "        vehicle_counts[category] += 1\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(f\"Camera {cam_id}\", image_with_boxes)\n",
    "\n",
    "# Print the counts of different vehicle categories with counts greater than 0\n",
    "for category, count in vehicle_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{category}: {count}\")\n",
    "\n",
    "# Wait for a key press and then close OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af9c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
